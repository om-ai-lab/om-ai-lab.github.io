<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>VLM-R1 Blog</title>
    <link rel="stylesheet" href="css/index-styles.css">
</head>
<body>
    <header>
        <div class="header-container">
            <img src="images/OmAI-logo.v2.png" alt="OmAI Lab Logo" class="lab-logo">
            <div class="header-text">
                <h1>Om AI Lab Blogs</h1>
                <!-- <p>Official blog for the <a href="https://github.com/om-ai-lab/VLM-R1">VLM-R1 project</a> - Solving Visual Understanding with Reinforced VLMs</p> -->
            </div>
        </div>
    </header>

    <main>
        <ul class="blog-list">
            <li>
                <img src="images/0717/OPS5.png" alt="OmAgent Roadmap" class="blog-image">
                <div class="blog-content">
                    <span class="date">July 17, 2025</span>
                    <h2><a href="2025_07_17_en.html">OmAgent - A Reinforcement Learning-based Multimodal Agent Framework</a></h2>
                    <p>With the rapid advancement of Large Language Models (LLMs) and Vision Language Models (VLMs), AI technology is shifting from exam-oriented task completion to practical scenario-based complex problem-solving. Using LLMs and VLMs to tackle more realistic and intricate problems—rather than simply passing exams—is not only an inevitable direction of technological evolution but also a key requirement for industrial applications. We launched the first embodied AI agent—OmAgent, a reinforcement learning-based multimodal agent framework. Its feasibility has been verified in practical applications.</p>
                    <a href="2025_07_17_en.html" class="read-more">Read more →</a>
                </div>
            </li>
            <li>
                <img src="images/0717/OPS5.png" alt="OmAgent特点" class="blog-image">
                <div class="blog-content">
                    <span class="date">2025年7月17日</span>
                    <h2><a href="2025_07_17_zh.html">OmAgent - 基于强化学习的多模态智能体</a></h2>
                    <p>随着大型语言模型（LLMs）与视觉语言模型（VLMs）的能力飞速发展，AI 技术正从「应试式」的任务达标转向「实战化」的复杂问题解决。用LLMs和VLMs去解决更实际更复杂的问题，而不是简单地通过“考试”， 这既是技术演进的必然方向，也是产业落地的核心诉求。我们推出全球首个具身智能 AI Agent——OmAgent，一个基于强化学习的多模态智能体框架，并在实际应用中验证了该路径的可行性。​</p>
                    <a href="2025_07_17_zh.html" class="read-more">Read more →</a>
                </div>
            </li>
            <li>
                <img src="images/3b-accurancy.png" alt="3B vs 7B Model Comparison" class="blog-image">
                <div class="blog-content">
                    <span class="date">March 24, 2025</span>
                    <h2><a href="2025_03_24.html">Trials, Errors, and Breakthroughs: Our Rocky Road to OVD SOTA with Reinforcement Learning</a></h2>
                    <p>Key insights from our extensive experimentation with Reinforcement Learning for object detection in Vision Language Models, focusing on training methodologies, reward functions, and prompt engineering.</p>
                    <a href="2025_03_24.html" class="read-more">Read more →</a>
                </div>
            </li>
            <li>
                <img src="images/coco_overall.v3.png" alt="COCO results" class="blog-image">
                <div class="blog-content">
                    <span class="date">March 20, 2025</span>
                    <h2><a href="2025_03_20.html">Improving Object Detection through Reinforcement Learning with VLM-R1</a></h2>
                    <p>A detailed exploration of how reinforcement learning enhances object detection performance compared to supervised fine-tuning in vision-language models.</p>
                    <a href="2025_03_20.html" class="read-more">Read more →</a>
                </div>
            </li>
            <!-- Additional blog posts would go here -->
        </ul>
    </main>
</body>
</html> 